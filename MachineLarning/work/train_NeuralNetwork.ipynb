{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setproctitle\n",
    "setproctitle.setproctitle('train_NeuralNetwork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# GPU(CUDA)が使えるかどうか？\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.backends.cudnn.deterministic = False  # 非決定論的である代わりに高速化\n",
    "torch.backends.cudnn.benchmark = True       # 画像サイズが変化しない場合に高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data list from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, csv\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import select\n",
    "from database.settings import DBManager\n",
    "from database.tables   import MNIST, Uploaded\n",
    "\n",
    "db = DBManager()\n",
    "engine = create_engine(f\"{db.dialect}://{db.username}:{db.password}@{db.host}:{db.port}/{db.database}?charset=utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "newdata_filename='./data/newdata_list.csv'\n",
    "try:\n",
    "\tdf = pd.read_sql(sql=select(Uploaded), con=engine)\n",
    "\tdf['relpath'] = f\"{Uploaded.__tablename__}/\" + df['relpath']\n",
    "\n",
    "except Exception as err:\n",
    "\tif \"Table\" and \"doesn't exist\" in err.args[0]:\n",
    "\t\tprint(f\"No data in {db.database}/{Uploaded.__tablename__}\")\n",
    "\telse:\n",
    "\t\traise\n",
    "else:\n",
    "\tif df.shape[0] > 0:\n",
    "\t\tprint(f\"Loading data from {db.host}/{Uploaded.__tablename__}\")\n",
    "\t\tos.makedirs(os.path.join('data/', Uploaded.__tablename__), exist_ok=True)\n",
    "\t\t\n",
    "\t\tnewId = df[ [ not x for x in df['is_used']] ]\n",
    "\t\tnewId = newId['id']\n",
    "\t\tnewId.to_csv(newdata_filename, header=False, index=False)\n",
    "\t\timages = df.loc[:,['relpath','label']]\n",
    "\telse:\n",
    "\t\tprint(f\"No data in {db.database}/{Uploaded.__tablename__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_sql(sql=select(MNIST), con=engine)\n",
    "df['relpath'] = f\"{MNIST.__tablename__}/\" + df['relpath']\n",
    "\n",
    "print(f\"Loading data from {db.host}:{db.database}/{MNIST.__tablename__}\")\n",
    "os.makedirs(os.path.join('data/', f'{MNIST.__tablename__}'), exist_ok=True)\n",
    "if 'images' in locals():\n",
    "\timages = pd.concat([ images, df.loc[:,['relpath','label']] ])\n",
    "else:\n",
    "\timages = df.loc[:,['relpath','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンジン破棄\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data from the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool, Manager\n",
    "workers = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from storage.settings import StorageManager\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from botocore.config     import Config\n",
    "\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "storage = StorageManager()\n",
    "def init():\n",
    "\tglobal bucket\n",
    "\ts3 = boto3.resource(\n",
    "\t\tservice_name          = \"s3\",\n",
    "\t\tendpoint_url          = f\"http://{storage.host}:{storage.port}\",\n",
    "\t\taws_access_key_id     = storage.username,\n",
    "\t\taws_secret_access_key = storage.password,\n",
    "\t\tconfig                = Config(max_pool_connections=workers,\n",
    "\t\t\t\t\t\t\t\t\t   proxies={'http':  os.getenv('HTTP_PROXY'),\n",
    "\t\t\t\t\t\t\t\t\t   \t\t\t'https': os.getenv('HTTPS_PROXY')})\n",
    "\t)\n",
    "\tbucket = s3.Bucket(storage.bucket)\n",
    "\n",
    "\tglobal session\n",
    "\tengine  = create_engine(f\"{db.dialect}://{db.username}:{db.password}@{db.host}:{db.port}/{db.database}?charset=utf8\")\n",
    "\tsession = Session(autocommit=False,\n",
    "\t\t\t\t\t   autoflush=True,\n",
    "\t\t\t\t\t   expire_on_commit=False,\n",
    "\t\t\t\t\t   bind=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = multiprocessing.Manager()\n",
    "notFoundList = manager.list()\n",
    "def download_images(relpath):\n",
    "\tlocalpath = os.path.join('./data', relpath)\n",
    "\tif not os.path.isfile(localpath):\n",
    "\t\ttry:\n",
    "\t\t\tbucket.download_file(Key=relpath, Filename=localpath)\n",
    "\t\texcept ClientError as e:\n",
    "\t\t\tif e.response['Error']['Code'] == \"404\":\n",
    "\t\t\t\t# print(f\"The object \\\"{relpath}\\\" does not exist.\")\n",
    "\t\t\t\tdirname  = os.path.dirname(relpath)\n",
    "\t\t\t\tbasename = os.path.basename(relpath)\n",
    "\t\t\t\tentry = session.query(Uploaded).filter_by(relpath=f'{basename}').first()\n",
    "\t\t\t\tsession.delete(entry)\n",
    "\t\t\t\tnotFoundList.append(relpath)\n",
    "\t\t\t\tsession.commit()\n",
    "\t\t\telse:\n",
    "\t\t\t\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\tprint(f\"Downloading images from the data storage...\")\n",
    "\twith Pool(processes=workers, initializer=init) as parallel:\n",
    "\t\tparallel.map(download_images, images.loc[:,'relpath'])\n",
    "\t\tflag = images['relpath'].isin(list(notFoundList))\n",
    "\t\tflag = [not x for x in flag]\n",
    "\t\timages = images[flag]\n",
    "\t\tcsv_filename ='./data/data_list.csv'\n",
    "\t\timages.to_csv(csv_filename, header=False, index=False)\n",
    "\tprint(f\"Download completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_definition import Net\n",
    "#----------------------------------------------------------\n",
    "# ニューラルネットワークの生成\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_dataset import MyDataset\n",
    "full_dataset = MyDataset(\n",
    "\tcsv_file=csv_filename,\n",
    "\troot_dir='./data',\n",
    "\ttransform=model.transform\n",
    ")\n",
    "\n",
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size  = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "\tfull_dataset, [train_size, test_size]\n",
    ")\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 学習用／評価用のデータセットの作成\n",
    "# ハイパーパラメータなどの設定値\n",
    "num_epochs = 10         # 学習を繰り返す回数\n",
    "num_batch = 100         # 一度に処理する画像の枚数\n",
    "learning_rate = 0.001   # 学習率\n",
    "\n",
    "# データローダー\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "\ttrain_dataset,\n",
    "\tbatch_size = num_batch,\n",
    "\tshuffle = True)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "\ttest_dataset,\n",
    "\tbatch_size = num_batch,\n",
    "\tshuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#----------------------------------------------------------\n",
    "# 学習\n",
    "model.train()  # モデルを訓練モードにする\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "print(f\"Start training.\")\n",
    "epoch_losses = []\n",
    "for epoch in range(num_epochs): # 学習を繰り返し行う\n",
    "\tlosses = []\n",
    "\n",
    "\tfor inputs, labels in train_dataloader:\n",
    "\t\t# GPUが使えるならGPUにデータを送る\n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t# # optimizerを初期化\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# # ニューラルネットワークの処理を行う\n",
    "\t\toutputs = model(inputs)\n",
    "\n",
    "\t\t# # 損失(出力とラベルとの誤差)の計算\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tlosses.append(loss.item())\n",
    "\t\t# loss_sum += loss\n",
    "\n",
    "\t\t# # 勾配の計算\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t# # 重みの更新\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t# 学習状況の表示\n",
    "\tepoch_losses.append( mean(losses) )\n",
    "\tprint(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {epoch_losses[-1]}\")\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "# モデルの重みの保存\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t_delta = datetime.timedelta(hours=9)\n",
    "JST = datetime.timezone(t_delta, \"JST\")\n",
    "now = datetime.datetime.now(JST)\n",
    "os.makedirs('log', exist_ok=True)\n",
    "with open(now.strftime(\"log/%Y%m%d%H%M%S_losses.txt\"), \"w\") as file:\n",
    "\tfile.writelines([ str(x)+'\\n' for x in epoch_losses ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# 評価\n",
    "model.eval()  # モデルを評価モードにする\n",
    "\n",
    "losses = []\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "\tfor inputs, labels in test_dataloader:\n",
    "\t\t# GPUが使えるならGPUにデータを送る\n",
    "\t\tinputs = inputs.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t# ニューラルネットワークの処理を行う\n",
    "\t\toutputs = model(inputs)\n",
    "\n",
    "\t\t# 損失(出力とラベルとの誤差)の計算\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tlosses.append(loss.item())\n",
    "\n",
    "\t\t# 正解の値を取得\n",
    "\t\tpred = outputs.argmax(1)\n",
    "\t\t# 正解数をカウント\n",
    "\t\tcorrect += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "print(f\"Loss: {mean(losses)}, Accuracy: {100*correct/len(test_dataset)}% ({correct}/{len(test_dataset)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(newdata_filename):\n",
    "\twith open(newdata_filename, 'r') as newdata_file:\n",
    "\t\treader = csv.reader(newdata_file)\n",
    "\n",
    "\t\tengine = create_engine(f\"{db.dialect}://{db.username}:{db.password}@{db.host}:{db.port}/{db.database}?charset=utf8\")\n",
    "\t\tsession = Session(engine)\n",
    "\t\tfor id in reader:\n",
    "\t\t\tnewdata = session.query(Uploaded).filter(id=id[0]).first()\n",
    "\t\t\tnewdata.is_used = True\n",
    "\t\tsession.commit()\n",
    "\t\tsession.close()\n",
    "\t\tengine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
